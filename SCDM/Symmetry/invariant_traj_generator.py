import os
import gym
import time
import dexterous_gym
import numpy as np


class InvariantTrajGenerator():
    def __init__(self, args):
        self.delay = args.delay
        self.env = gym.make(args.env)
        self.files = os.listdir("../TOPDM/prerun_trajectories/" + args.env)
        self.traj_prefix = "../TOPDM/prerun_trajectories/" + args.env + "/"
        self.invariant_prefix = "../TOPDM/invariant_trajectories/" + args.env + "/invariant_"

    def inv_traj_render(self, traj, traj_reflected, render_with_simulation=True):
        # testing reflected_traj
        if render_with_simulation:
            # this flag indicates setting the initial states with the traj[0] and executing the actions
            # render original traj
            print('------render original traj with simulation------')
            self.env.reset()
            self.env.env.sim.set_state(traj["sim_states"][0])
            self.env.goal = traj["goal"]
            self.env.env.goal = traj["goal"]
            self.env.render()
            for action in traj["actions"]:
                self.env.step(action)
                time.sleep(self.delay)
                self.env.render()

            #  render reflected traj
            print('------render reflected traj with simulation------')
            self.env.reset()
            self.env.env.sim.set_state(traj_reflected["sim_states"][0])
            self.env.goal = traj_reflected["goal"]
            self.env.env.goal = traj_reflected["goal"]
            self.env.render()
            for action in traj_reflected["actions"]:
                self.env.step(action)
                time.sleep(self.delay)
                self.env.render()
        else:
            # we can only use this to debug qpos, but it is not applicable for qvel
            print('------render original traj without simulation------')
            self.env.reset()
            self.env.goal = traj["goal"]
            self.env.env.goal = traj["goal"]
            for t in range(len(traj["sim_states"])):
                self.env.env.sim.set_state(traj["sim_states"][t])
                self.env.render()
                time.sleep(self.delay)

            print('------render reflected traj without simulation------')
            self.env.reset()
            self.env.goal = traj_reflected["goal"]
            self.env.env.goal = traj_reflected["goal"]
            for t in range(len(traj_reflected["sim_states"])):
                self.env.env.sim.set_state(traj_reflected["sim_states"][t])
                self.env.render()
                time.sleep(self.delay)

    def compare_real_state_with_artificial_state(self, traj_invariant, reset_everytime=False):
        # real states: the states generated by executing actions in the simulation
        # artificial states: the states generated by the invariant_trajectory_generator
        self.env.reset()
        self.env.env.sim.set_state(traj_invariant["sim_states"][0])
        self.env.env.sim.forward()
        timestep=0
        print('------start to check the difference------')
        for action in traj_invariant["actions"]:
            if reset_everytime:
                self.env.env.sim.set_state(traj_invariant["sim_states"][timestep])
                self.env.env.sim.forward()
            self.env.step(action)
            timestep = timestep + 1
            real_state = self.env.env.sim.data.qpos
            obs = self.env.env._get_obs()
            # print(real_state[0:3])
            # print(traj_reflected["sim_states"][timestep].qpos[0:3])
            # print(real_state[30:33])
            # print(traj_reflected["sim_states"][timestep].qpos[30:33])
            if reset_everytime:
                print('error every step: ', np.linalg.norm(real_state - traj_invariant["sim_states"][timestep].qpos))
            else:
                print('cumulative error: ', np.linalg.norm(real_state[0:6] - traj_invariant["sim_states"][timestep].qpos[0:6]))

    def recording(self, args, traj):
        from gym.wrappers import record_video
        env = record_video.RecordVideo(gym.make(args.env), video_folder='video/', name_prefix='invariant_traj_in_sim')
        # render
        env.reset()
        env.env.sim.set_state(traj["sim_states"][0])
        env.goal = traj["goal"]
        env.env.goal = traj["goal"]
        env.render()
        for action in traj["actions"]:
            env.step(action)
            time.sleep(self.delay)
            env.render()


